version: '3.8'

services:
  # Core services
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11435:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - lama-network
    restart: unless-stopped

  # PyLama - Central orchestration service
  pylama:
    build:
      context: ./pylama
      dockerfile: Dockerfile
    ports:
      - "9003:8003"
    volumes:
      - ./pylama:/app
    environment:
      - PYTHONUNBUFFERED=1
      - PORT=8003
      - HOST=0.0.0.0
      - BEXY_API_URL=http://bexy:8000
      - PYLLM_API_URL=http://pyllm:8001
      - SHELLAMA_API_URL=http://apilama:8080/api/shellama
      - OLLAMA_API_URL=http://ollama:11434
    depends_on:
      - ollama
      - bexy
      - pyllm
      - apilama
    networks:
      - lama-network
    restart: unless-stopped

  # API Gateway
  apilama:
    build:
      context: ./apilama
      dockerfile: Dockerfile
    ports:
      - "9080:8080"
    volumes:
      - ./apilama:/app
      - ./weblama/markdown:/app/markdown
    environment:
      - PYTHONUNBUFFERED=1
      - PORT=8080
      - HOST=0.0.0.0
      - BEXY_API_URL=http://bexy:8000
      - PYLLM_API_URL=http://pyllm:8001
      - PYLAMA_API_URL=http://pylama:8003
      - SHELLAMA_API_URL=http://shellama:8002
      - MARKDOWN_DIR=/app/markdown
    depends_on:
      - bexy
      - pyllm
      - shellama
    networks:
      - lama-network
    restart: unless-stopped

  # Shell Operations Service
  shellama:
    build:
      context: ./shellama
      dockerfile: Dockerfile
    ports:
      - "9002:8002"
    volumes:
      - ./shellama:/app
      - ./data:/data  # Shared data directory for file operations
    environment:
      - PYTHONUNBUFFERED=1
      - PORT=8002
      - HOST=0.0.0.0
    networks:
      - lama-network
    restart: unless-stopped

  # Execution services
  bexy:
    build:
      context: ./bexy
      dockerfile: Dockerfile
    ports:
      - "9000:8000"
    volumes:
      - ./bexy:/app
    environment:
      - PYTHONUNBUFFERED=1
      - PORT=8000
      - HOST=0.0.0.0
    networks:
      - lama-network
    restart: unless-stopped

  pyllm:
    build:
      context: ./pyllm
      dockerfile: Dockerfile
    ports:
      - "9001:8001"
    volumes:
      - ./pyllm:/app
    environment:
      - PYTHONUNBUFFERED=1
      - PORT=8001
      - HOST=0.0.0.0
    depends_on:
      - ollama
    networks:
      - lama-network
    restart: unless-stopped

  # Frontend (Static Web Interface)
  weblama:
    build:
      context: ./weblama
      dockerfile: Dockerfile.nginx
    ports:
      - "9081:80"
    volumes:
      - ./weblama/static:/usr/share/nginx/html
      - ./weblama/markdown:/usr/share/nginx/html/markdown
    environment:
      - API_URL=http://apilama:8080
      - API_PORT=8080
      - API_HOST=apilama
      - MARKDOWN_DIR=/usr/share/nginx/html/markdown
    depends_on:
      - apilama
    networks:
      - lama-network
    restart: unless-stopped

networks:
  lama-network:
    driver: bridge

volumes:
  ollama_data:
