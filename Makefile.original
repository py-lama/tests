# Makefile for PyLama Microservices

# Default values for environment variables
PORT ?= 8080
HOST ?= 127.0.0.1

# Port assignments for each service
BEXY_PORT ?= 8000
GETLLM_PORT ?= 8001
SHELLAMA_PORT ?= 8002
PYLAMA_PORT ?= 8003
APILAMA_PORT ?= 8080
WEBLAMA_PORT ?= 80

.PHONY: all setup clean test lint format docker-build docker-up docker-down run-bexy run-getllm run-pylama run-apilama run-shellama run-weblama run-all monitor help docker-test docker-test-bexy docker-test-getllm docker-test-apilama docker-test-weblama docker-test-pylama docker-integration-test

# Default target
all: help

# Setup all projects
setup: setup-bexy setup-getllm setup-pylama setup-apilama setup-shellama setup-weblama

# Setup individual projects with virtual environments
setup-bexy:
	@echo "Setting up BEXY..."
	cd bexy && python -m venv venv && . venv/bin/activate && pip install -e .

setup-getllm:
	@echo "Setting up PyLLM..."
	cd getllm && python -m venv venv && . venv/bin/activate && pip install -e .

setup-pylama:
	@echo "Setting up PyLama (Central Orchestration Service)..."
	cd pylama && python -m venv venv && . venv/bin/activate && pip install -e .

setup-apilama:
	@echo "Setting up APILama (API Gateway)..."
	cd apilama && python -m venv venv && . venv/bin/activate && pip install -e .

setup-shellama:
	@echo "Setting up SheLLama (Shell Operations)..."
	cd shellama && python -m venv venv && . venv/bin/activate && pip install -e .

setup-weblama:
	@echo "Setting up WebLama (Frontend)..."
	cd weblama && npm install

# Clean all projects
clean: clean-bexy clean-getllm clean-pylama clean-apilama clean-shellama clean-weblama
	@echo "Cleaning up Docker resources..."
	docker-compose down -v
	@echo "Removing logs..."
	rm -f service_status.log

# Clean individual projects
clean-bexy:
	@echo "Cleaning BEXY..."
	find bexy -type d -name __pycache__ -exec rm -rf {} +
	find bexy -type d -name *.egg-info -exec rm -rf {} +
	find bexy -type d -name .pytest_cache -exec rm -rf {} +


clean-getllm:
	@echo "Cleaning PyLLM..."
	find getllm -type d -name __pycache__ -exec rm -rf {} +
	find getllm -type d -name *.egg-info -exec rm -rf {} +
	find getllm -type d -name .pytest_cache -exec rm -rf {} +

clean-pylama:
	@echo "Cleaning PyLama..."
	find pylama -type d -name __pycache__ -exec rm -rf {} +
	find pylama -type d -name *.egg-info -exec rm -rf {} +
	find pylama -type d -name .pytest_cache -exec rm -rf {} +

clean-weblama:
	@echo "Cleaning WebLama..."
	find weblama -type d -name __pycache__ -exec rm -rf {} +
	find weblama -type d -name *.egg-info -exec rm -rf {} +
	find weblama -type d -name .pytest_cache -exec rm -rf {} +

# Clean APILama
clean-apilama:
	@echo "Cleaning APILama..."
	find apilama -type d -name __pycache__ -exec rm -rf {} +
	find apilama -type d -name *.egg-info -exec rm -rf {} +
	find apilama -type d -name .pytest_cache -exec rm -rf {} +
	find apilama -type d -name venv -exec rm -rf {} +

# Clean SheLLama
clean-shellama:
	@echo "Cleaning SheLLama..."
	find shellama -type d -name __pycache__ -exec rm -rf {} +
	find shellama -type d -name *.egg-info -exec rm -rf {} +
	find shellama -type d -name .pytest_cache -exec rm -rf {} +
	find shellama -type d -name venv -exec rm -rf {} +

# Run tests for all projects
test: test-bexy test-getllm test-pylama test-apilama test-shellama test-weblama

# Test individual projects
test-bexy:
	@echo "Testing BEXY..."
	cd bexy && python -m pytest tests/

test-getllm:
	@echo "Testing PyLLM..."
	cd getllm && python -m pytest tests/

test-pylama:
	@echo "Testing PyLama..."
	cd pylama && python -m pytest tests/

test-weblama:
	@echo "Testing WebLama..."
	cd weblama && python -m unittest discover

test-apilama:
	@echo "Testing APILama..."
	cd apilama && . venv/bin/activate && python -m unittest discover

test-shellama:
	@echo "Testing SheLLama..."
	cd shellama && . venv/bin/activate && python -m unittest discover

# Lint all projects
lint: lint-bexy lint-getllm lint-pylama lint-apilama lint-shellama lint-weblama

# Lint individual projects
lint-bexy:
	@echo "Linting BEXY..."
	flake8 bexy

lint-getllm:
	@echo "Linting PyLLM..."
	flake8 getllm

lint-pylama:
	@echo "Linting PyLama..."
	flake8 pylama

lint-weblama:
	@echo "Linting WebLama..."
	flake8 weblama

# Format all projects
format: format-bexy format-getllm format-pylama format-weblama

# Format individual projects
format-bexy:
	@echo "Formatting BEXY..."
	black bexy

format-getllm:
	@echo "Formatting PyLLM..."
	black getllm

format-pylama:
	@echo "Formatting PyLama..."
	black pylama

format-weblama:
	@echo "Formatting WebLama..."
	black weblama

# Docker commands
docker-build:
	@echo "Building Docker images..."
	docker-compose build

docker-up:
	@echo "Starting Docker containers..."
	docker-compose up -d

docker-down:
	@echo "Stopping Docker containers..."
	docker-compose down

# Docker testing commands
docker-test: docker-test-bexy docker-test-getllm docker-test-apilama docker-test-weblama docker-test-pylama
	@echo "All Docker tests completed."

docker-test-bexy:
	@echo "Running BEXY tests in Docker..."
	cd bexy && make docker-test

docker-test-getllm:
	@echo "Running PyLLM tests in Docker..."
	cd getllm && make docker-test

docker-test-apilama:
	@echo "Running APILama tests in Docker..."
	cd apilama && make docker-test

docker-test-weblama:
	@echo "Running WebLama tests in Docker..."
	cd weblama && make docker-test

docker-test-pylama:
	@echo "Running PyLama tests in Docker..."
	cd pylama && make docker-test

docker-integration-test:
	@echo "Running integration tests across all components..."
	cd pylama && make docker-integration

# Run individual services
run-bexy:
	@echo "Running BEXY on port $(BEXY_PORT)..."
	cd bexy && . venv/bin/activate && python -m bexy.app --port $(BEXY_PORT) --host $(HOST)

run-getllm:
	@echo "Running PyLLM on port $(GETLLM_PORT)..."
	cd getllm && . venv/bin/activate && python -m getllm.app --port $(GETLLM_PORT) --host $(HOST)

run-shellama:
	@echo "Running SheLLama on port $(SHELLAMA_PORT)..."
	cd shellama && . venv/bin/activate && python -m shellama.app --port $(SHELLAMA_PORT) --host $(HOST)

run-pylama:
	@echo "Running PyLama (Central Orchestration) on port $(PYLAMA_PORT)..."
	cd pylama && . venv/bin/activate && python -m pylama.app --port $(PYLAMA_PORT) --host $(HOST)

run-apilama:
	@echo "Running APILama (API Gateway) on port $(APILAMA_PORT)..."
	cd apilama && . venv/bin/activate && python -m apilama.app --port $(APILAMA_PORT) --host $(HOST)

run-weblama:
	@echo "Running WebLama (Frontend) on port $(WEBLAMA_PORT)..."
	cd weblama && npm start -- --port $(WEBLAMA_PORT)

# Run all services (in background)
run-all:
	@echo "Running all services in the correct order..."
	mkdir -p logs
	# Start services in the correct dependency order with PyLama as the central orchestration point
	@echo "Starting BEXY (Sandbox) on port $(BEXY_PORT)..."
	cd bexy && . venv/bin/activate && python -m bexy.app --port $(BEXY_PORT) --host $(HOST) > ../logs/bexy.log 2>&1 &
	@echo "Starting PyLLM (LLM Operations) on port $(GETLLM_PORT)..."
	cd getllm && . venv/bin/activate && python -m getllm.app --port $(GETLLM_PORT) --host $(HOST) > ../logs/getllm.log 2>&1 &
	@echo "Starting SheLLama (Shell Operations) on port $(SHELLAMA_PORT)..."
	cd shellama && . venv/bin/activate && python -m shellama.app --port $(SHELLAMA_PORT) --host $(HOST) > ../logs/shellama.log 2>&1 &
	@echo "Starting APILama (API Gateway) on port $(APILAMA_PORT)..."
	cd apilama && . venv/bin/activate && python -m apilama.app --port $(APILAMA_PORT) --host $(HOST) > ../logs/apilama.log 2>&1 &
	@echo "Starting PyLama (Central Orchestration) on port $(PYLAMA_PORT)..."
	cd pylama && . venv/bin/activate && python -m pylama.app --port $(PYLAMA_PORT) --host $(HOST) > ../logs/pylama.log 2>&1 &
	@echo "Starting WebLama (Frontend) on port $(WEBLAMA_PORT)..."
	cd weblama && npm start -- --port $(WEBLAMA_PORT) > ../logs/weblama.log 2>&1 &
	@echo "All services started. Check logs directory for output."
	@echo "Access the web interface at http://localhost:$(WEBLAMA_PORT)"

# Run the monitoring script
monitor:
	@echo "Running monitoring script..."
	python monitor_services.py

# Deploy the system
deploy: docker-build docker-up
	@echo "System deployed and running. Run 'make monitor' to check status."

# Help
help:
	@echo "PyLama Microservices Makefile"
	@echo ""
	@echo "Available targets:"
	@echo "  setup       - Set up all projects"
	@echo "  clean       - Clean all projects"
	@echo "  test        - Run tests for all projects"
	@echo "  lint        - Lint all projects"
	@echo "  format      - Format all code with black"
	@echo ""
	@echo "Docker commands:"
	@echo "  docker-build - Build Docker images"
	@echo "  docker-up   - Start Docker containers (RECOMMENDED deployment method)"
	@echo "  docker-down - Stop Docker containers"
	@echo ""
	@echo "Docker testing commands:"
	@echo "  docker-test  - Run tests for all components in Docker"
	@echo "  docker-test-bexy   - Run BEXY tests in Docker"
	@echo "  docker-test-getllm   - Run PyLLM tests in Docker"
	@echo "  docker-test-apilama - Run APILama tests in Docker"
	@echo "  docker-test-weblama - Run WebLama tests in Docker"
	@echo "  docker-test-pylama  - Run PyLama tests in Docker"
	@echo "  docker-integration-test - Run integration tests across all components"
	@echo ""
	@echo "Individual services (for development):"
	@echo "  run-pylama  - Run PyLama (Central Orchestration Service) on port $(PYLAMA_PORT)"
	@echo "  run-apilama - Run APILama (API Gateway) on port $(APILAMA_PORT)"
	@echo "  run-shellama - Run SheLLama (Shell Operations) on port $(SHELLAMA_PORT)"
	@echo "  run-bexy   - Run BEXY (Sandbox) on port $(BEXY_PORT)"
	@echo "  run-getllm   - Run PyLLM (LLM Operations) on port $(GETLLM_PORT)"
	@echo "  run-weblama - Run WebLama (Frontend) on port $(WEBLAMA_PORT)"
	@echo ""
	@echo "Combined operations:"
	@echo "  run-all     - Run all services in the correct order with PyLama as central point"
	@echo "  monitor     - Run the monitoring script to check service health"
	@echo "  deploy      - Deploy the entire system using Docker (recommended)"
	@echo "  help        - Show this help message"
	@echo ""
	@echo "Individual project targets are also available with -bexy, -getllm, -pylama, -apilama, -shellama, or -weblama suffixes"
	@echo "Example: make setup-bexy"
